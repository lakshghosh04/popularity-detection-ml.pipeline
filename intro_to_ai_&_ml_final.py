# -*- coding: utf-8 -*-
"""Intro to AI & ML Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s--SuZyn2BLASh9VgFBa9hGLdwWxX05s

About The Company & How Can This Help The Our Company?

IMDb (Internet Movie Database) well defined by it's name most renouned website for movie ratings; as their newly appointed data scientist i've built an end-to-end machine learning pipeline which pridicts the popularity of a movie on the bases of some given factors and by utlizing this we can introduce a Movie Recommendation System which could enhance the user experience and benefit industry stakeholders, including filmmakers, studios, distributors, and advertisers.

#Submission Form
"""

from IPython.display import Image
image_path = "/content/Submission Form ML .png"
Image(filename=image_path)

"""##Libirary"""

import pandas as pd
from sklearn.model_selection import train_test_split,  RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
from scipy.stats import randint

"""##Dataset Loading [ https://www.kaggle.com/datasets/adriankiezun/imdb-dataset-2023?rvi=1 ]"""

df = pd.read_csv("/content/imdb_data.csv")
df

"""## Data Exploration

shape function helps you to know the shape of your dataset like here i have 3348 rows and 12 column.
"""

df.shape

df.info()

""" To check if there is any missing values for each column"""

missing_values = df.isnull().sum()
print(missing_values)

""" Checking if there is any duplicate rows"""

duplicate_rows = df.duplicated().sum()
print("Number of duplicate rows:", duplicate_rows)

df.describe()

"""## Data Pre-Processing

Dropping un-wanted columns for a clean dataset & Filling missing values with median numbers
"""

df = df.drop(['id', 'primaryTitle', 'directors'], axis=1)

df['gross'].fillna(df['gross'].median(), inplace=True)
df

"""#Feature Engineering

Categorizing ratings
"""

df['rating_category'] = (df['averageRating'] >= 8.0).astype(int)

"""Selecting features and target variable"""

x = df[['runtimeMinutes', 'numVotes', 'budget', 'gross', 'genres', 'release_date']]
y = df['rating_category']

"""#Data Spliting

spliting the data into train and test
"""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

"""#Scaling and Encoding within a Column Transformer

One-hot encoding the 'genres' column and scaling numerical features
"""

categorical_features = ['genres']
numerical_features = ['runtimeMinutes', 'numVotes', 'budget', 'gross']

scaling = ColumnTransformer(
    transformers=[
        ('standard_scaling', StandardScaler(), numerical_features),
        ('one_hot-encoding', OneHotEncoder(handle_unknown='ignore'), categorical_features),
    ]
)

"""#Training the Model

Creating a pipeline that preprocesses and then appliying on the model
"""

lr = Pipeline(steps=[('scaling', scaling),
                           ('classifier', LogisticRegression(max_iter=1000))])

lr.fit(x_train, y_train)

"""#Model Evaluation"""

y_prid = lr.predict(x_test)

print(f"Accuracy: {accuracy_score(y_test, y_prid)}")
print(f"Precision: {precision_score(y_test, y_prid)}")
conf_matrix = confusion_matrix(y_test, y_prid)

"""Visualizing our confusion matrix"""

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

"""Trying To Improve My Results With RandomForrestClassifier and Hyperp

Creating the pipeline for RFC
"""

pipeline = Pipeline([
    ('preprocessing', ColumnTransformer([
        ('numerical', StandardScaler(), numerical_features),
        ('categorical', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])),
    ('model', RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42))
])

pipeline.fit(x_train, y_train)

"""Hyperp training to select the best machine learning algorithm and tune its hyper-
parameters.

Defining hyperparameters for randomized search
"""

hyperp = {
    'model__n_estimators': randint(100, 500),
    'model__max_depth': [None] + list(randint(5, 50).rvs(10)),
    'model__min_samples_split': randint(2, 20),
    'model__min_samples_leaf': randint(1, 20)
}

"""Performing randomized-search for hyperparameter tuning and getting the best model from randomized search"""

rndm = RandomizedSearchCV(pipeline, param_distributions=hyperp, n_iter=20, cv=5, scoring='accuracy', random_state=42)
rndm.fit(x_train, y_train)

best_model = rndm.best_estimator_

"""Predicting for the test set"""

y_prid = pipeline.predict(x_test)

"""Evaluating RandomForrestClassifier"""

accuracy = accuracy_score(y_test, y_prid)
precision = precision_score(y_test, y_prid)
recall = recall_score(y_test, y_prid)
f1 = f1_score(y_test, y_prid)

print(f"Best Parameters: {rndm.best_params_}")
print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

"""# Conclusion

Over-all this pipeline is performing well with both training models, However the scores of logistic regression is better still RFC is not bad. This Model pridicts the popularity of movies and company can use this to enhance the user experience and introduce some wall of fame and showcase the best picks.

Counting down some strengths:

1.   Flexiblity to dataset:
   this is capable to accommodate different types of models and preprocessing techniques.
2.   Performance and interpretability:
Both RFC and Logistic Regression model achieve high accuracy and precision on this test set, indicating their effectiveness
    

Counting down some limitations:

1.   Scalability : with some extremely big dataset, the pipeline's performance may degrade. more advanced tuning will be required and will take time.

2.   Complexity and FeatureEng. RFC can be complex: RFc is complicated in coparesion to logistic regression and the effectiveness of this modeel heavily relies on the selection and enginering of features.

Recomendations:

Continious enhancement as there are new movies and new rating the dataset should be upgraded and updated.

Reference: [ https://stackoverflow.com ]
"""